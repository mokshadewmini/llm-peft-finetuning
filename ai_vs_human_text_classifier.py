# -*- coding: utf-8 -*-
"""AI_vs_Human_Text_Classifier

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qYjfYmVQhGQc0m5rPLSUPXfVi94U0Qra
"""

# Install dependencies
!pip install transformers datasets peft accelerate bitsandbytes -q

import pandas as pd

# Sample data: 10 AI-generated and 10 Human-written examples
data = {
    "text": [
        "The weather today is partly cloudy with a chance of rain in the evening.",
        "AI has the potential to revolutionize many industries by automating tasks.",
        "We must remain vigilant in protecting our planet and preserving biodiversity.",
        "Your payment has been successfully processed. Thank you for your order!",
        "Sustainable agriculture involves practices that maintain the health of the environment.",
        "This article explores the fundamentals of neural networks and deep learning.",
        "In this chapter, we will discuss the impact of social media on mental health.",
        "This document was generated using artificial intelligence language models.",
        "Experience a new way of working with our intelligent automation solutions.",
        "Quantum computing is a rapidly evolving field that may redefine computation.",
        "I went to the market yesterday and bought fresh vegetables for dinner.",
        "My friend and I took a long walk by the river and enjoyed the sunset.",
        "He forgot to bring his umbrella, so he got completely drenched in the rain.",
        "We celebrated her birthday with a surprise party and homemade cake.",
        "The dog barked loudly when it saw the squirrel run across the fence.",
        "She studied hard for the exam and managed to get an A in the end.",
        "Every weekend, I like to clean my room and do all the laundry.",
        "My brother always forgets where he keeps his headphones.",
        "They watched a movie last night and then played some board games.",
        "He reads a book every night before going to sleep."
    ],
    "label": [
        "AI", "AI", "AI", "AI", "AI", "AI", "AI", "AI", "AI", "AI",
        "Human", "Human", "Human", "Human", "Human", "Human", "Human", "Human", "Human", "Human"
    ]
}

# Convert to DataFrame and shuffle
df = pd.DataFrame(data).sample(frac=1).reset_index(drop=True)
df.head()

from transformers import AutoTokenizer

# Use a small model for quick training
model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

# Encode the dataset
def tokenize(example):
    return tokenizer(example["text"], truncation=True, padding="max_length", max_length=128)

# Convert to Hugging Face Dataset
from datasets import Dataset
dataset = Dataset.from_pandas(df, preserve_index=True)

# Encode text
tokenized_dataset = dataset.map(tokenize)

# Convert labels to numerical
label2id = {"AI": 0, "Human": 1}
tokenized_dataset = tokenized_dataset.map(lambda x: {"label": label2id[x["label"]]})
tokenized_dataset = tokenized_dataset.remove_columns(["text"])
tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.2)

tokenized_dataset

import torch
from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer
from peft import get_peft_model, LoraConfig, TaskType

# Load base model
base_model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)

# Setup LoRA config
peft_config = LoraConfig(
    task_type=TaskType.SEQ_CLS,
    inference_mode=False,
    r=8,
    lora_alpha=16,
    lora_dropout=0.1
)

# Apply PEFT (LoRA) to the model
model = get_peft_model(base_model, peft_config)

# Training arguments
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    num_train_epochs=5,
    logging_dir='./logs',
    logging_steps=10,
    load_best_model_at_end=True
)

# Define Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
    compute_metrics=compute_metrics # Add compute_metrics here
)

# Start training
trainer.train()

import torch
from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer
from peft import get_peft_model, LoraConfig, TaskType

# Define LoRA configuration specifically for DistilBERT
peft_config = LoraConfig(
    task_type=TaskType.SEQ_CLS,  # Sequence Classification
    inference_mode=False,
    r=8,                         # LoRA rank
    lora_alpha=16,
    lora_dropout=0.1,
    target_modules=["q_lin", "v_lin"]  # Key for DistilBERT
)

# Define Trainer
trainer = Trainer(
    model=model,

    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"]
)

# Apply the LoRA configuration to your base model
model = get_peft_model(base_model, peft_config)

# Show model summary (optional)
model.print_trainable_parameters()
trainer.train()

trainer.evaluate()

trainer.save_model("lora-distilbert-finetuned")

from transformers import pipeline

pipe = pipeline("text-classification", model="lora-distilbert-finetuned", tokenizer=tokenizer)
pipe("The service was excellent and I loved the food!")

from huggingface_hub import login
login()

from transformers import AutoModelForSequenceClassification, AutoTokenizer
from peft import PeftModel

base_model_name = "distilbert-base-uncased"
model = AutoModelForSequenceClassification.from_pretrained(base_model_name, num_labels=2)
model = PeftModel.from_pretrained(model, "lora-distilbert-finetuned")

model.push_to_hub("dewminimoksha/lora-distilbert-finetuned")
tokenizer.push_to_hub("dewminimoksha/lora-distilbert-finetuned")

!pip install matplotlib-venn

!apt-get -qq install -y libfluidsynth1
# https://pypi.python.org/pypi/libarchive
!apt-get -qq install -y libarchive-dev && pip install -U libarchive
import libarchive
# https://pypi.python.org/pypi/pydot
!apt-get -qq install -y graphviz && pip install pydot
import pydot
!pip install cartopy
import cartopy

# Assuming you have already tokenized your dataset
# Perform train-test split manually (if needed)
tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.2)

train_dataset = tokenized_dataset["train"]
test_dataset = tokenized_dataset["test"]



!pip install evaluate -q

# ✅ STEP 1: Import the correct evaluation library
from evaluate import load
from transformers import Trainer

# ✅ STEP 2: Load the metric (accuracy, f1, precision, etc.)
metric = load("accuracy")

# ✅ STEP 3: Define the compute_metrics function for evaluation
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = logits.argmax(axis=1)
    return metric.compute(predictions=preds, references=labels)

# ✅ STEP 4: Create test_dataset from tokenized_datasets
# If you split your dataset already, this should work:
test_dataset = tokenized_dataset["test"]

# ✅ STEP 5: Evaluate the model
results = trainer.evaluate(
    eval_dataset=test_dataset,
    metric_key_prefix="eval",
    # compute_metrics=compute_metrics # Remove compute_metrics here
)

# ✅ STEP 6: Print the results
print("Evaluation Results:", results)

import gradio as gr
from transformers import pipeline # Import pipeline from transformers

# Initialize the sentiment analysis pipeline globally
# You can choose a different sentiment model if you prefer, e.g., "distilbert-base-uncased-finetuned-sst-2-english"
# For a general text classification, you might use another model.
pipe = pipeline("sentiment-analysis") # This will download the model the first time

def classify_sentiment(text):
    if not text: # Handle empty input
        return "Please enter some text."
    try:
        # The pipeline returns a list of dictionaries, e.g., [{'label': 'POSITIVE', 'score': 0.999}]
        result = pipe(text)
        # Extract the label and score
        label = result[0]['label']
        score = result[0]['score']
        return f"Sentiment: {label} (Score: {score:.2f})"
    except Exception as e:
        return f"An error occurred: {e}"

# If you're running this in Colab, share=True is automatically set,
# which generates a public URL for your Gradio app.
gr.Interface(fn=classify_sentiment, inputs="text", outputs="text", title="AI vs. Human Text Classifier (Sentiment)").launch()

print("Evaluating on test set...")
results = trainer.evaluate(test_dataset)
print(results)

# Get a few examples from the test set and show predictions
test_dataset_with_index = tokenized_dataset["test"]

for i in range(len(test_dataset_with_index)):
    # Access each example in the dataset
    example = test_dataset_with_index[i]
    # Get the original text from the DataFrame
    text = df.iloc[example['__index_level_0__']]['text']
    true_label = example["label"]

    # Prepare the input for the model
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding="max_length", max_length=128).to(model.device)

    # Get model predictions
    with torch.no_grad():
        outputs = model(**inputs)

    # Get predicted label
    predicted_label_id = outputs.logits.argmax().item()
    predicted_label = list(label2id.keys())[list(label2id.values()).index(predicted_label_id)]

    print(f"\n--- Example {i+1} ---")
    print(f"Text:\n{text}")
    print(f"True Label:\n{list(label2id.keys())[true_label]}") # Convert numerical label back to string
    print(f"Predicted Label:\n{predicted_label}")



print(tokenized_dataset["test"].features)

import gradio as gr
from transformers import pipeline # Import pipeline from transformers

# Initialize the sentiment analysis pipeline globally
# You can choose a different sentiment model if you prefer, e.g., "distilbert-base-uncased-finetuned-sst-2-english"
# For a general text classification, you might use another model.
pipe = pipeline("sentiment-analysis") # This will download the model the first time

def classify_sentiment(text):
    if not text: # Handle empty input
        return "Please enter some text."
    try:
        # The pipeline returns a list of dictionaries, e.g., [{'label': 'POSITIVE', 'score': 0.999}]
        result = pipe(text)
        # Extract the label and score
        label = result[0]['label']
        score = result[0]['score']
        return f"Sentiment: {label} (Score: {score:.2f})"
    except Exception as e:
        return f"An error occurred: {e}"

# If you're running this in Colab, share=True is automatically set,
# which generates a public URL for your Gradio app.
gr.Interface(fn=classify_sentiment, inputs="text", outputs="text", title="AI vs. Human Text Classifier (Sentiment)").launch()

